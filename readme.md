# üå∏ Project Nova

## A Fully Local AI Companion with Memory, Emotion, Voice, Avatar Control, and Agentic Abilities

**Project Nova** is a fully local, real-time AI companion designed to feel persistent, emotionally aware, and interactive.  
It combines a fine-tuned large language model, long-term memory, expressive text-to-speech, speech recognition, and live 3D avatar control ‚Äî all optimized to run on consumer hardware.

---
>### üîî Updates
>### v0.1 ‚Äî Discord Integration (Latest)
>- Added **Discord bot support** to chat with Nova remotely
>- Uses the same **LLM, memory, and personality** system
>- Secure token handling via **`.env`** (not committed)
>- Can run alongside the **local voice/avatar version**
- #### Scroll below to see the instructions for setting up Nova as a Discord Bot. 

---
> ‚ö†Ô∏è **Development Disclaimer**
>
> Project Nova was **not created using ‚Äúvibe coding‚Äù or fully AI-generated code**.
>
> The system architecture, core logic, threading model, memory design, agentic behavior, and debugging were **designed, implemented, and validated manually**.
> 
>Time Taken for development (approx 2 months)
>
> AI tools were used **only as a support resource** for:
> - Understanding unfamiliar concepts (eg. Threading, RAG)
> - Exploring alternative approaches (eg. Chunking, Streaming TTS)
> - Identifying potential issues
>
> When automated suggestions failed or were incorrect, **all debugging, fixes, and final decisions were performed manually**.
>
> This project reflects hands-on engineering, iterative testing, and deliberate system design ‚Äî not prompt-to-project generation.


## ‚ú® Core Features

### üß† Long-Term Memory (RAG-Based)

- Persistent memory powered by **ChromaDB**
- Semantic embeddings enable **retrieval-augmented generation (RAG)**
- Supports:
  - Explicit memories (user asks Nova to remember)
  - Episodic / emotional memories
  - Contextual recall during conversation
- Latest memories are injected at startup to maintain continuity across sessions

> Memory is semantic, persistent, and local ‚Äî no hard-coded state.

---

 ### ü§ñ Language Model (LLM)

- Uses a **Phi-3.5 fine-tuned model**  (4B parameters, Q4_K_M quantization)
- Fine-tuned on a **custom conversational dataset using Google Collab**
- Optimized for
  - Structured JSON output
  - Consistent personality
  - Low-latency local inference
- Runs locally via [Ollama](https://ollama.com)
- Enforced **structured JSON output**:
- Model is available at [Hugging Face](https://huggingface.co/Navpy/phi-3.5-AI-Vtuber-json)

```json
{
  "response": "text",
  "emotion": "emotion_name"
}
```

- Low-latency
- VRAM-efficient
- Optimized for real-time interaction
---
### üó£Ô∏è Speech-to-Text (STT)
- Powered by Faster-Whisper
- Fully local and offline transcription
- Real-time microphone input
- Optimized for low latency and accuracy
---
### üîä Text-to-Speech (TTS) Architecture
Nova‚Äôs TTS system is designed for low latency, emotional expressiveness, and smooth playback ‚Äî even on limited VRAM.
### üß© TTS Model
- Soprano-TTS (80M param, lightweight, local)
- GPU-accelerated using CUDA
- Emotion-aware generation via controlled parameters:
    - temperature
    - top_p
    - repetition_penalty
### üß† Text Processing
Output text is cleaned to remove patterns that commonly cause hallucinations or unstable TTS output:
- Ellipses (..., ‚Ä¶‚Ä¶)
- Stutters (I...I'll)
- Excess punctuation
- Newlines and malformed tokens
### ‚öôÔ∏è Chunking + Streaming System (Producer‚ÄìConsumer)
Nova does not generate and play audio in a blocking way. Instead, she uses a producer‚Äìconsumer threading architecture.
1. **Chunking** : Response text is split into natural sentence chunks to prevent long generations and enable early playback.
2. **Producer Thread (Audio Generation)** : Generates audio chunks sequentially and pushes them into a shared queue.
3. **Consumer Thread (Audio Streaming)** : Continuously reads from the queue and streams to the output device. Playback begins before full generation completes.
#### ‚úÖ Results: Low perceived latency, natural pauses, and stable audio.
---
### üé≠ Emotional Expression System
Each LLM response includes an emotion tag that controls:
- üéôÔ∏è Voice generation parameters
- üßç Facial expressions
- üï∫ Idle animations
#### **Behavior** : Emotion is applied at speech start and automatically released when playback ends.
---
### üßç 3D Avatar Control (VMC Protocol)
- Uses **VMC / VSeeFace** protocol
- Sends real-time data for:
    - Facial expressions
    - Head movement
    - Emotional states
- Avatar reacts synchronously with voice and emotion
---
### üéß Audio Routing & Lip-Sync
- Audio output routed through **Virtual Audio Cable**
- VSeeFace uses the virtual cable as microphone **input**
- Enables accurate **real-time lip-sync**
---
### üïí Time Awareness
- Natural language date & time awareness
- Conversations and memories are timestamped
- Nova can reference time contextually
---
### ü§ñ Agentic Abilities
**Nova can perform real system actions based on user intent:**
- üéµ Play songs on YouTube (via songs.json)
- üåê Open websites (Google, YouTube, etc.)
- üóÇÔ∏è Perform basic file operations
- üß† Decide when to act vs respond conversationally
> ‚ö†Ô∏è Some agentic actions (e.g., file operations) are intentionally limited and rule-based to prevent unintended behavior.
#### **All agentic logic is:** Fully local, rule-based, and explicitly triggered.
---
### üß† System Architecture
```
User
 ‚Üì
Speech-to-Text (Faster-Whisper)
 ‚Üì
Phi-3.5 Fine-Tuned LLM (Ollama)
 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Memory System ‚îÇ ‚Üê ChromaDB (RAG)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 ‚Üì
Structured JSON Output
 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   TTS    ‚îÇ  Avatar    ‚îÇ Agentic  ‚îÇ
‚îÇ (Chunks) ‚îÇ (VMC)      ‚îÇ Actions  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 ‚Üì
Audio ‚Üí Virtual Audio Cable ‚Üí VSeeFace
```
---
## üíª System Requirements
### Hardware
- **NVIDIA GPU:** (4‚Äì6 GB VRAM minimum)
## üß™ Development & Minimum Hardware Target

Project Nova was **designed, developed, tested, and optimized** on the following hardware:

- **GPU:** NVIDIA RTX 2050  
- **VRAM:** 4 GB  
- **CUDA:** Enabled  
- **Platform:** Windows

This configuration represents the **minimum recommended hardware** to run Project Nova smoothly with:

- Local LLM inference (Phi-3.5 fine-tuned, quantized)
- GPU-accelerated TTS
- Real-time speech recognition
- Avatar control via VMC
- Concurrent agentic features

All architectural decisions ‚Äî including **model selection, quantization, chunked TTS streaming, and producer‚Äìconsumer threading** ‚Äî were made specifically to ensure stable performance within a **4 GB VRAM constraint**.

> If the project runs reliably on this system, it is expected to scale better on higher-end GPUs.

### Software
- Windows (recommended)
- [Ollama](https://ollama.com/)
- [Python 3.11](https://www.python.org/downloads/) (recommended)
- [CUDA Toolkit: (12.x or newer)](https://developer.nvidia.com/cuda-downloads)
- NVIDIA Drivers
- [VSeeFace](https://www.vseeface.icu/#download)
- [Virtual Audio Cable **(VAC)**](https://vb-audio.com/Cable/)
---
## üì¶ Installation
### 1Ô∏è‚É£ Clone Repository
```bash
git clone https://github.com/Navjot-Singh7/Project-Nova.git
cd Project Nova
```
### 2Ô∏è‚É£ Create Virtual Environment
```bash
py -3.11 -m venv my_env

my_env\Scripts\activate
```
### 3Ô∏è‚É£ Install PyTorch (Required)
#### Example for NVIDIA GPU (CUDA 12.6):
```bash
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu126
```
### 4Ô∏è‚É£ Install Dependencies
```bash
pip install -r requirements.txt
```
### 5Ô∏è‚É£ Install Model directly from HuggingFace to your downloads folder (Recommended)
https://huggingface.co/Navpy/phi-3.5-AI-Vtuber-json/tree/main
### 6Ô∏è‚É£ Fix Modelfile for your system
***Note*** - Copy the path where your model is downloaded and paste it in the Modelfile which is inside **assistant_modelfile/** folder
```bash
# make sure it is inside quotes
FROM "<YOUR_MODEL_FILE_PATH>"
```
### 7Ô∏è‚É£ Create the Model - open your terminal in the folder where you have cloned the repository and run this command
```bash
# Make sure you have ollama installed in your computer
ollama create ai-vtuber -f assistant_modelfile/Modelfile
```
### 8Ô∏è‚É£ Check if the model is created or not
**Note** - You'll see your model name in the terminal
```bash
ollama list
```
### 9Ô∏è‚É£ Install Text Embedding model - this model is used for Retrieval Augmented Generation (RAG)
```bash
ollama pull nomic-embed-text
```
### üîü Run the main Script
```bash
python main.py
```
---
### üéôÔ∏è Audio Setup
1. Install a **Virtual Audio Cable**.
2. Set Nova‚Äôs output device to the virtual cable.
3. Set **VSeeFace** microphone input to the same virtual cable.
#### Then press window+r on your keyboard and follow these instructions so that lip syncing will work
- Type "mmsys.cpl"
- Go to **Recording tab**
- You'll see **CABLE OUTPUT** go to it's properties
- Go to **Listen tab**
- Select **"Listen to this device"** and choose your default output deivce
- Press **Apply**
- Now your lipsync will work
---
### üß† Memory Storage
Stored locally in:
```waifu_memories/```
- Persistent across sessions
- Ignored by Git
- Auto-created if missing
---
### üîê Privacy & Offline Use
- **Fully local:** No cloud APIs.
- **Works offline:** After models are downloaded.
- **Privacy:** No Data is sent to any cloud servers
---

## üí¨ Discord Integration (Optional)

Project Nova can also be accessed via a **Discord bot interface**, allowing you to chat with Nova remotely while preserving her personality and memory system.

### Features
- Text-based conversation with Nova on Discord
- Uses the **same LLM and memory system** as the local companion
- Supports long-term memory and contextual responses
- Can run alongside the local voice/avatar version or independently

> The Discord bot is an optional interface.  
> Nova‚Äôs core intelligence remains fully local.
---
### üõ†Ô∏è Discord Bot Setup

1. Create a Discord application:
   - Go to https://discord.com/developers/applications
   - Create a new application
   - Add a **Bot** and copy the bot token

2. Create a `.env` file in the project root:
```bash
DISCORD_BOT_TOKEN="<your_bot_token_here>"
```
3. Invite the bot to your server using the OAuth2 URL from the Developer Portal.
4. Install Discord dependencies
```
pip install discord.py python-dotenv
```
5. Run the Discord bot script
```
python bot.py
```

### ‚ö†Ô∏è Disclaimer
Project Nova is an experimental AI companion project intended for:
- Learning
- Research
- Personal use
#### It is not a **substitute** for professional or emotional support.
---
> Project Nova is designed to explore how far a **fully local, persistent, emotionally-aware AI companion** can go ‚Äî without cloud dependence.
---
## ‚ù§Ô∏è Credits
- Ollama
- Soprano TTS - https://github.com/ekwek1/soprano
- ChromaDB
- Faster-Whisper
- VSeeFace
- Open-source AI community
---
## üìÑ License
This project is licensed under the **MIT License**.  
You are free to use, modify, and distribute this project for personal or educational purposes.

---
‚≠ê If you like this project, please give it a star!
---